#!/usr/bin/env python
"""Run ORCA jobs from a manifest with local parallelism and resume support."""

from __future__ import annotations

import argparse
import csv
import shutil
import subprocess
import time
from concurrent.futures import ThreadPoolExecutor, as_completed
from dataclasses import dataclass
from pathlib import Path

import pandas as pd


@dataclass(frozen=True)
class Job:
    candidate_id: str
    stage: str
    input_file: Path
    workdir: Path


def _terminated_normally(out_file: Path) -> bool:
    if not out_file.exists():
        return False
    text = out_file.read_text(encoding="utf-8", errors="ignore")
    return "ORCA TERMINATED NORMALLY" in text


def _run_single_job(job: Job, orca_bin: str, dry_run: bool) -> dict[str, object]:
    out_file = job.input_file.with_suffix(".out")
    err_file = job.input_file.with_suffix(".err")

    start = time.perf_counter()
    if dry_run:
        return {
            "candidate_id": job.candidate_id,
            "stage": job.stage,
            "input_file": str(job.input_file),
            "out_file": str(out_file),
            "status": "dry_run",
            "return_code": 0,
            "elapsed_s": 0.0,
        }

    cmd = [orca_bin, str(job.input_file.name)]
    with out_file.open("w", encoding="utf-8") as out_fh, err_file.open(
        "w", encoding="utf-8"
    ) as err_fh:
        proc = subprocess.run(
            cmd,
            cwd=job.workdir,
            stdout=out_fh,
            stderr=err_fh,
            check=False,
        )

    elapsed = time.perf_counter() - start
    ok = proc.returncode == 0 and _terminated_normally(out_file)
    return {
        "candidate_id": job.candidate_id,
        "stage": job.stage,
        "input_file": str(job.input_file),
        "out_file": str(out_file),
        "status": "ok" if ok else "failed",
        "return_code": int(proc.returncode),
        "elapsed_s": elapsed,
    }


def build_parser() -> argparse.ArgumentParser:
    p = argparse.ArgumentParser(description=__doc__)
    p.add_argument(
        "--manifest",
        default="runs/orca_jobs/orca_job_manifest.csv",
        help="Path to manifest generated by orca_prepare_inputs.py",
    )
    p.add_argument(
        "--orca-bin",
        default="orca",
        help="ORCA executable path or command name (default: orca)",
    )
    p.add_argument(
        "--max-workers",
        type=int,
        default=1,
        help="Maximum concurrent ORCA jobs (default: 1)",
    )
    p.add_argument(
        "--only-stage",
        default="",
        help="Optional comma-separated stage filter (e.g. vertical,s1opt)",
    )
    p.add_argument(
        "--resume",
        action="store_true",
        help="Skip jobs with existing .out containing ORCA TERMINATED NORMALLY",
    )
    p.add_argument(
        "--dry-run",
        action="store_true",
        help="Print plan and write summary without executing ORCA",
    )
    p.add_argument(
        "--summary-csv",
        default="runs/orca_jobs/orca_run_summary.csv",
        help="Output summary CSV path",
    )
    return p


def _load_jobs(manifest_path: Path, only_stage: set[str]) -> list[Job]:
    df = pd.read_csv(manifest_path)
    required = {"candidate_id", "stage", "input_file", "workdir"}
    missing = required - set(df.columns)
    if missing:
        raise ValueError(f"Manifest missing columns: {sorted(missing)}")
    if only_stage:
        df = df[df["stage"].isin(only_stage)].copy()
    jobs = []
    for _, row in df.iterrows():
        jobs.append(
            Job(
                candidate_id=str(row["candidate_id"]),
                stage=str(row["stage"]),
                input_file=Path(str(row["input_file"])),
                workdir=Path(str(row["workdir"])),
            )
        )
    return jobs


def main() -> int:
    args = build_parser().parse_args()
    manifest = Path(args.manifest)
    if not manifest.exists():
        raise SystemExit(f"Manifest not found: {manifest}")

    orca_bin = args.orca_bin
    if not args.dry_run:
        resolved = shutil.which(orca_bin) if not Path(orca_bin).exists() else orca_bin
        if not resolved:
            raise SystemExit(f"ORCA executable not found: {orca_bin}")
        orca_bin = resolved

    stage_filter = {s.strip() for s in args.only_stage.split(",") if s.strip()}
    jobs = _load_jobs(manifest, stage_filter)
    if not jobs:
        raise SystemExit("No jobs to run (after optional stage filtering).")

    to_run: list[Job] = []
    skipped_rows: list[dict[str, object]] = []
    for job in jobs:
        out_file = job.input_file.with_suffix(".out")
        if args.resume and _terminated_normally(out_file):
            skipped_rows.append(
                {
                    "candidate_id": job.candidate_id,
                    "stage": job.stage,
                    "input_file": str(job.input_file),
                    "out_file": str(out_file),
                    "status": "skipped_resume",
                    "return_code": 0,
                    "elapsed_s": 0.0,
                }
            )
            continue
        to_run.append(job)

    print(f"Jobs total={len(jobs)} | run={len(to_run)} | skipped={len(skipped_rows)}")
    if args.dry_run:
        for job in to_run[:20]:
            print(f"[dry-run] {job.stage:<9} {job.input_file}")
        if len(to_run) > 20:
            print(f"... {len(to_run)-20} additional jobs omitted")

    rows = list(skipped_rows)
    if to_run:
        max_workers = max(1, args.max_workers)
        with ThreadPoolExecutor(max_workers=max_workers) as ex:
            futures = [
                ex.submit(_run_single_job, job, orca_bin, args.dry_run) for job in to_run
            ]
            for fut in as_completed(futures):
                row = fut.result()
                rows.append(row)
                print(
                    f"{row['status']:<12} {row['stage']:<9} {row['candidate_id']} "
                    f"({row['elapsed_s']:.1f}s)"
                )

    summary_path = Path(args.summary_csv)
    summary_path.parent.mkdir(parents=True, exist_ok=True)
    with summary_path.open("w", newline="", encoding="utf-8") as fh:
        writer = csv.DictWriter(
            fh,
            fieldnames=[
                "candidate_id",
                "stage",
                "input_file",
                "out_file",
                "status",
                "return_code",
                "elapsed_s",
            ],
        )
        writer.writeheader()
        writer.writerows(rows)

    failed = sum(1 for r in rows if r["status"] == "failed")
    print(f"Wrote {summary_path} | failed={failed}")
    return 1 if failed else 0


if __name__ == "__main__":
    raise SystemExit(main())
